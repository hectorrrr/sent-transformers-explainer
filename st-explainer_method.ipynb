{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to clarify our own implementation of a 'Sentence Transformers' explainability method. \n",
    "For this we will be basing our work in two main sources:\n",
    "* The 'Integrated Gradients' method, a numerical method that stablish a cuantitative relation between inputs and outputs in Deep learning models. For further context refer to the [original paper](https://arxiv.org/pdf/1703.01365)\n",
    "\n",
    "* Captum, an Open source python library that implements multiple 'explainability' methods, on top of Pytorch models. Specially, we take as reference the [Bert Tutorial](https://captum.ai/tutorials/Bert_SQUAD_Interpret)\n",
    "\n",
    "\n",
    "\n",
    "We are going to do now a small mathematical review of the Integrated Gradients method, that will be necessary afterwards to understand every component of our method:\n",
    "\n",
    "As pointed in the Paper itself, this method 'aims' to  *Attribute the output (prediction) of a network to it's inputs*. This means that tries to 'decompose' the predicted value of the model in terms of the Input variables. This would be equivalent that each coefficient in a Linear Regression model. In this case, the idea behind the method is to get those 'Attributions', using a reference point in the space of inputs, called baseline, and compute a *path integral of the gradients along a straightline from the baseline x' to the input x* [(1)](https://arxiv.org/pdf/1703.01365):\n",
    "\n",
    "$$\n",
    "\\text{IntegratedGrads}(x) := (x - x') \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha \\times (x - x'))}{\\partial x_i} d\\alpha\n",
    "$$\n",
    "\n",
    "\n",
    "One of the most relevant and important consequences of the Integrated Gradients, is that, due to the properties of Path Integrals, the sum of the Attrobutions of each of the inputs have to correspond to the difference of the output between the chosen Baseline and the real input.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\text{IntegratedGrads}_i(x) = F(x) - F(x')\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Therefore, we can see that the Baseline that we choose is really influencing the method. Ideally we should pick one that satisfies F(x')~0, but we will see later that for most 'Sentence Transformers' models, this is not feasible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our model\n",
    "\n",
    "The first step will be to define our model. Even if we are using sentence transformers model, we will be using it throught their Pytorch backbone, because we need to be able to use the 'tokens' as inputs directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\sent-transformers-explainer\\captum_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load a pre-trained sentence-transformers model\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model.to(device)\n",
    "# model.eval()\n",
    "# model.zero_grad()\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a 'forward' function, that is going to generate the output of our model, and takes as input the exact inputs of the model. Therefore this inputs should be the tokens of our sentences, because those are the real inputs of our model and not a 'sentence'. This forward function will be used afterwards by the captum method to get the numerical gradients of the model transformation.\n",
    "\n",
    "Even if the tokens are already numerical features, we do know that they don't have a real numerical model. Therefore, we will be using LayerIntegratedGradients, that allows to get the 'Attributions' for each of the Inputs/Outputs of a specified layer of the model. In this case we are interested in getting the Attributions of the Embeddings layer, so we can get the relevance of each word (token) based in the Attributions of each dimension of their embeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def predict(input_ids, token_type_ids=None, attention_mask=None):\n",
    "    \"\"\"\n",
    "    Predicts the similarity between pairs of sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_ids: A tensor of shape [number_of_pairs, 2, seq_len] containing pairs of sentences.\n",
    "    - token_type_ids: Optional token type ids (same shape as input_ids).\n",
    "    - attention_mask: Optional attention mask (same shape as input_ids).\n",
    "    \n",
    "    Returns:\n",
    "    - similarities: A tensor of shape [number_of_pairs, 1] containing similarity scores for each pair.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_pairs = input_ids.shape[0]\n",
    "    seq_len = input_ids.shape[-1]\n",
    "    \n",
    "    # Flatten the input for batch processing\n",
    "    input_ids = input_ids.view(num_pairs * 2, seq_len)\n",
    "    \n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view(num_pairs * 2, seq_len)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.view(num_pairs * 2, seq_len)\n",
    "\n",
    "\n",
    "    # First we get the embeddings for all the inputs ids that have been flatten\n",
    "    model_output = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "    sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    \n",
    "    # Then we return them to their original shape (n_pairs,n_sentences, length)\n",
    "    sentence_embeddings = sentence_embeddings.view(num_pairs, 2, -1)\n",
    "    similarities = torch.nn.functional.cosine_similarity(sentence_embeddings[:, 0, :], sentence_embeddings[:, 1, :], dim=1)\n",
    "\n",
    "    return similarities.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_trans_pos_forward_func(input_ids,token_type_ids,attention_mask):\n",
    "    logging.info(f\"Method calling the predict function with shape {input_ids.shape}\")\n",
    "    pred = predict(input_ids,token_type_ids,attention_mask)\n",
    "    logging.info(f\"Value predicted---> {pred}\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input for the function and Baselines\n",
    "\n",
    "Now that we have configured our forward function, we are going to build several functions to get the inputs for this function.\n",
    "This was extracted from the Captum tutorial and slightly modified for our current use case. Let's go over them:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input IDs and Ref Input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(sentences, ref_token_id=0, process_both=True):\n",
    "    \"\"\"\n",
    "    Construct input_ids and ref_input_ids for a pair of sentences, excluding the first one\n",
    "    if specified.\n",
    "    \n",
    "    Input:\n",
    "    - sentences: A list of 2 sentences.\n",
    "    - ref_token_id: Token to be used as reference.\n",
    "    - process_both: Flag to determine if processing both phrases or just the second.\n",
    "    \n",
    "    Output:\n",
    "    - input_ids: Tensor of shape [1, 2, max_length]\n",
    "    - ref_input_ids: Tensor of shape [1, 2, max_length]\n",
    "    \"\"\"\n",
    "\n",
    "    # We could generalize this method to any dimension of pairs of sentences\n",
    "    assert len(sentences) == 2\n",
    "\n",
    "    # Tokenize the pair\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids']  # shape: [2, max_length]\n",
    "    \n",
    "    # Create a reference tensor using the ref token\n",
    "    ref_input_ids = torch.full(input_ids.shape, ref_token_id)\n",
    "    \n",
    "    # Reintroduce the classification and sep tokens\n",
    "    for i, input_id_sequence in enumerate(input_ids):\n",
    "        if not process_both and i == 0:  # Skip processing for the first sentence if process_both is False\n",
    "            ref_input_ids[i] = input_id_sequence\n",
    "        else:\n",
    "            ref_input_ids[i, input_id_sequence == cls_token_id] = cls_token_id\n",
    "            ref_input_ids[i, input_id_sequence == sep_token_id] = sep_token_id\n",
    "\n",
    "    # Reshape input_ids and ref_input_ids to add an additional level of depth [1, 2, max_length]\n",
    "    input_ids = input_ids.unsqueeze(0)  # shape: [1, 2, max_length]\n",
    "    ref_input_ids = ref_input_ids.unsqueeze(0)  # shape: [1, 2, max_length]\n",
    "\n",
    "    return input_ids, ref_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 16])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input tensor (batch_size=2, 2 pairs per batch, seq_length=16)\n",
    "input_ids = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "        [  101,  1045,  2293,  5983,  2003,  2026,  5440,  7570, 10322,  2666,  1012,   102,     0,     0,     0,     0],\n",
    "        [  101,  1045,  2293,  5983,  2003,  2026,  5440,  7570, 10322,  2600,  1012,   102,  0,     0,     0,     0]\n",
    "        ],\n",
    "        [\n",
    "        [  101,  1045,  2293,  5983,  2003,  2026,  5440,  7570, 10322,  2666,  1012,   102,     0,     0,     0,     0],\n",
    "        [  101,  1045,  2293,  5983,  2,  2026,  5440,  7570, 10322,  2600,  1012,   102,  0,     0,     0,     0]\n",
    "        ]\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities: tensor([[0.9198],\n",
      "        [0.8362]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming token_type_ids and attention_mask are similar in structure\n",
    "token_type_ids = torch.zeros_like(input_ids)  # For simplicity, assume all zeros\n",
    "attention_mask = (input_ids != 0).long()  # Attention mask where 0s are padding\n",
    "\n",
    "# Assuming `model` and `mean_pooling` are defined, and predict function is implemented\n",
    "cosine_similarities = predict(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "# Print the output\n",
    "print(\"Cosine similarities:\", cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking new things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This are the sentences to be compared\", \"more sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_token_type_ids(input_ids):\n",
    "    # Assume no token type differentiation if not needed\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "    ref_token_type_ids = torch.zeros_like(input_ids)\n",
    "    \n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_position_ids_pair(input_ids):\n",
    "    batch_size, pair_size, seq_length = input_ids.size()  # Get the size of each dimension\n",
    "    \n",
    "    # Create position IDs for each sequence in the pair (0, 1, ..., seq_length - 1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Expand position_ids to match the shape of input_ids: [batch_size, pair_size, seq_length]\n",
    "    position_ids = position_ids.expand(batch_size, pair_size, seq_length)\n",
    "    \n",
    "    # Create reference position IDs filled with zeros (same shape as position_ids)\n",
    "    ref_position_ids = torch.zeros_like(position_ids)\n",
    "    \n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model[0].auto_model.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model[0].auto_model.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  101,  2023,  2024,  1996, 11746,  2000,  2022,  4102,   102],\n",
       "         [  101,  2062, 11746,   102,     0,     0,     0,     0,     0]]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, ref_input_ids = construct_input_ref_pair(sentences,process_both=False)\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_type_ids, ref_token_type_ids = construct_token_type_ids(input_ids)\n",
    "\n",
    "position_ids, ref_position_ids = construct_position_ids_pair(input_ids)\n",
    "\n",
    "attention_mask = construct_attention_mask(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  101,  2023,  2024,  1996, 11746,  2000,  2022,  4102,   102],\n",
       "         [  101,     0,     0,   102,     0,     0,     0,     0,     0]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_sentence = \"I do love tenis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 18:25:06,821 - INFO - Method calling the predict function with shape torch.Size([1, 2, 9])\n",
      "2024-10-05 18:25:06,880 - INFO - Value predicted---> tensor([[0.5634]])\n",
      "2024-10-05 18:25:06,883 - INFO - Method calling the predict function with shape torch.Size([1, 2, 9])\n",
      "2024-10-05 18:25:06,894 - INFO - Value predicted---> tensor([[0.1246]])\n",
      "2024-10-05 18:25:06,930 - INFO - Method calling the predict function with shape torch.Size([100, 2, 9])\n",
      "2024-10-05 18:25:07,185 - INFO - Value predicted---> tensor([[0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1246],\n",
      "        [0.1247],\n",
      "        [0.1247],\n",
      "        [0.1247],\n",
      "        [0.1248],\n",
      "        [0.1249],\n",
      "        [0.1250],\n",
      "        [0.1251],\n",
      "        [0.1252],\n",
      "        [0.1254],\n",
      "        [0.1256],\n",
      "        [0.1258],\n",
      "        [0.1261],\n",
      "        [0.1264],\n",
      "        [0.1268],\n",
      "        [0.1272],\n",
      "        [0.1277],\n",
      "        [0.1283],\n",
      "        [0.1290],\n",
      "        [0.1298],\n",
      "        [0.1308],\n",
      "        [0.1318],\n",
      "        [0.1331],\n",
      "        [0.1345],\n",
      "        [0.1362],\n",
      "        [0.1381],\n",
      "        [0.1403],\n",
      "        [0.1428],\n",
      "        [0.1457],\n",
      "        [0.1491],\n",
      "        [0.1531],\n",
      "        [0.1577],\n",
      "        [0.1631],\n",
      "        [0.1696],\n",
      "        [0.1773],\n",
      "        [0.1866],\n",
      "        [0.1980],\n",
      "        [0.2117],\n",
      "        [0.2284],\n",
      "        [0.2483],\n",
      "        [0.2715],\n",
      "        [0.2978],\n",
      "        [0.3265],\n",
      "        [0.3562],\n",
      "        [0.3858],\n",
      "        [0.4139],\n",
      "        [0.4396],\n",
      "        [0.4623],\n",
      "        [0.4820],\n",
      "        [0.4985],\n",
      "        [0.5122],\n",
      "        [0.5232],\n",
      "        [0.5321],\n",
      "        [0.5392],\n",
      "        [0.5448],\n",
      "        [0.5492],\n",
      "        [0.5526],\n",
      "        [0.5553],\n",
      "        [0.5574],\n",
      "        [0.5591],\n",
      "        [0.5604],\n",
      "        [0.5614],\n",
      "        [0.5622],\n",
      "        [0.5628],\n",
      "        [0.5633],\n",
      "        [0.5636],\n",
      "        [0.5639],\n",
      "        [0.5641],\n",
      "        [0.5642],\n",
      "        [0.5643],\n",
      "        [0.5644],\n",
      "        [0.5644],\n",
      "        [0.5644],\n",
      "        [0.5643],\n",
      "        [0.5643],\n",
      "        [0.5642],\n",
      "        [0.5642],\n",
      "        [0.5641],\n",
      "        [0.5640],\n",
      "        [0.5640],\n",
      "        [0.5639],\n",
      "        [0.5638],\n",
      "        [0.5638],\n",
      "        [0.5637],\n",
      "        [0.5636],\n",
      "        [0.5636],\n",
      "        [0.5635],\n",
      "        [0.5635],\n",
      "        [0.5635],\n",
      "        [0.5634],\n",
      "        [0.5634],\n",
      "        [0.5634]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(sent_trans_pos_forward_func, model.embeddings)\n",
    "\n",
    "attributions_start = lig.attribute(inputs = input_ids,\n",
    "                                  baselines = ref_input_ids,\n",
    "                                  additional_forward_args=(token_type_ids, attention_mask),\n",
    "                                  return_convergence_delta=False,\n",
    "                                  n_steps =100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0898, 0.3490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_start.sum(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
